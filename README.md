
## Contacts

[Telegram chat](https://t.me/ml_se21)

**Lecturers**: [Anna Kuzina](https://akuzina.github.io/); [Evgenii Egorov](https://evgenii-egorov.github.io/)

**Class Teachers and TAs**

| Class Teachers | Contact | Group| TA (contact)|  
|----------------|---------|------|-------|
|Maria Tikhonova|tg: @mashkka_t|БПИ184|Alexandra Kogan (tg: @horror_in_black)|
|Maksim Karpov|tg: @buntar29|БПИ181, БПИ182 |Kirill Bykov (tg: @darkydash), Victor Grishanin (tg: @vgrishanin)|
|Polina Polinuna|tg: @ppolunina|БПИ185|Michail Kim (tg: @kimihailv)|
|Vadim Kokhtev|tg: @despairazure|БПИ183|Daniil Kosakin (tg: @nieto95)|


Use [this form](https://forms.gle/KeGbnntmsPcQXzhX6) to send feedback to the course team anytime 

## Recomended Literature

[PR] Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.\
[Link](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

[ESL] Hastie, T., Hastie, T., Tibshirani, R., & Friedman, J. H. (2001). The elements of statistical learning: Data mining, inference, and prediction. New York: Springer.\
[Link](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)

[FML] Mohri, M., Talwalkar, A., & Rostamizadeh, A. Second Edition, (2018). Foundations of Machine Learning. Cambridge, MA: The MIT Press.\
[Link](https://cs.nyu.edu/~mohri/mlbook/)

## Class materials

#### Lectures

[Lecture Recordings](https://eduhseru-my.sharepoint.com/:f:/g/personal/kroslovtseva_hse_ru/EsszFtVh8qdOuM_S2xhHYtIBdMyX2qaI6QGMwax-2AoTTQ?e=2O0OeF)

| Date | Topic | Lecture materials| Reading|
|------|-------|------------------|--------|
|30 jan|Introduction| [Slides](lectures/lecture1_intro.pdf) |[FML] Ch 1; [ESL] Ch 2.1-2.2 |
|6 feb|Gradient Optimization| [Slides](lectures/lecture2_gd.pdf) | [FML] Appx A, B; [Convex Optimization book](https://web.stanford.edu/~boyd/cvxbook/)|
|13 feb|Linear Regression| [Slides](lectures/lecture_3.slides.html), [Notebook](lectures/lecture_3.ipynb) |   |
|20 feb|Linear Classification|  ||   
|27 feb|Logistic Regression and SVM|  |   |
|6 mar|Decision Trees|  |   |
|13 mar|Bagging, Random Forest|  |   |
|20 mar|Gradient boosting|  |   |

#### Practicals

| Date | Topic | Materials| Extra Reading/Practice|  
|------|-------|----------|-----------------------|
|25-30 jan|Basic toolbox| [Notebook](practicals/Seminar_1/01_HSE_PE_Intro_to_Python_v4.ipynb); [Dataset](https://drive.google.com/drive/folders/1LeZ6JutPcRELcTi198AJe2n0tvgh_AAD?usp=sharing)|[Python Crash Course](practicals/Seminar_1/Additional_notebooks/)|
|1-6 feb|EDA and Scikit-learn| [Notebook](practicals/Seminar_2/02_HSE_SE_EDA_v1.ipynb) ||
|8-13 feb|Calculus recap and Gradient Descent| [Notebook](practicals/Seminar_3/sem03-gd.ipynb), [pdf](practicals/Seminar_3/sem03-vector-diff.ipynb) |[The Matrix Cookbook](http://www.math.uwaterloo.ca/~hwolkowi//matrixcookbook.pdf)|
|15-20 feb|Linear Regression|  ||
|22-27 feb|Classification|  ||
|1-6 mar|Texts and Multiclass classification|  ||
|8-13 mar|Decision Trees|  ||
|15-20 mar|Ensambles|  ||

## Assignments

We'll be using AnyTask for grading: [course link](https://anytask.org/course/769) 

| Date Published| Task | Deadline | 
|----------------|---------|---------|
|  6 feb  |HW 1: [Notebook](hw/hw_1/task.ipynb), [dataset](hw/hw_1/titanic.csv)| 20 feb|
|  20 feb |HW 2: `TBA`| 6 mar|
|  6 mar  |HW 3: `TBA`| 20 mar|
|  20 mar |HW 4: `TBA`| 10 apr|
|  24 apr |HW 5: `TBA`| 15 may|
|  29 may |HW 6 (Optional): `TBA`| 19 jun|



## Grading
```Final grade = 0.7*HW + 0.3*Exam```

* `HW` - Average grade for the assignments 1 to 5. 
You can get extra points by solving HW 6, but no more than 10 in total. 
* `Exam` -  Grade for the exam
 
 ---
 
You can skip the exam if your average grade for the first 5 assignemnts is **not smaller** than 6 (`HW >=6`). 
In this case:

```Final grade = HW```
